{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import hatchet as ht\n",
    "import spotdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in a single data file and visualize the tree and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to a SPOT/Caliper file\n",
    "input_deploy_dir_str = \"./data/cali-demo\"\n",
    "input_run_ids_str = \"cDPu64825TuLB5ujG_0.cali\" #problem size=10, iter=215, jobsize=1\n",
    "\n",
    "db = spotdb.connect(input_deploy_dir_str)\n",
    "runs = input_run_ids_str.split(',')\n",
    "\n",
    "# Read SPOT/Caliper file into a Hatchet GraphFrame\n",
    "gfs = ht.GraphFrame.from_spotdb(db, runs)\n",
    "gf = gfs.pop()\n",
    "\n",
    "launchdate = dt.datetime.fromtimestamp(int(gf.metadata[\"launchdate\"]))\n",
    "jobsize = int(gf.metadata.get(\"jobsize\", 1))\n",
    "\n",
    "print(\"launchdate: {}, jobsize: {}\".format(launchdate, jobsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree representation using the default metric\n",
    "print(gf.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the man page of different parameters for Hatchet's tree function\n",
    "help(gf.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe of metrics in HTML-format\n",
    "display(HTML(gf.dataframe.to_html()))\n",
    "\n",
    "# Alternatively, use standard printing of the dataframe\n",
    "#print(gf.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column to the dataframe transforming the inclusive time column to a percentage of the max inclusive time\n",
    "max_time = gf.dataframe[\"time (inc)\"].max()\n",
    "gf.dataframe[\"pct-of-max\"] = gf.dataframe[\"time (inc)\"] / max_time\n",
    "\n",
    "# Print the dataframe of metrics in HTML-format\n",
    "display(HTML(gf.dataframe.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the tree to contain only nodes consuming at least 60% of max time\n",
    "filter_func = lambda x: x[\"pct-of-max\"] > 0.6\n",
    "filtered_squashed_gf = gf.filter(filter_func,\n",
    "                                 squash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare size of input graph and filtered graph\n",
    "print(f\"Input Graph Size                       : {len(gf.graph)}\")\n",
    "print(f\"Result (after filter/squash) Graph Size: {len(filtered_squashed_gf.graph)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting tree\n",
    "print(filtered_squashed_gf.tree(metric_column=\"pct-of-max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calulate percent change between nightly test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to SPOT/Caliper files\n",
    "input_deploy_dir_str = \"./hatchet-data\"\n",
    "# Input file configurations:\n",
    "#     problemsize=50, iter=800, jobsize=343, 3/24/21 12:36\n",
    "#     problemsize=50, iter=800, jobsize=343, 3/24/21 12:45\n",
    "input_run_ids_str = \"cQ-CGJlYj-uFT2yv-_1.cali,cQ-CGJlYj-uFT2yv-_2.cali\"\n",
    "\n",
    "db = spotdb.connect(input_deploy_dir_str)\n",
    "runs = input_run_ids_str.split(',')\n",
    "\n",
    "# Read SPOT/Caliper files into a Hatchet GraphFrame\n",
    "gfs = ht.GraphFrame.from_spotdb(db, runs)\n",
    "\n",
    "gf = gfs[0]   # 3/24/21 12:36\n",
    "gf2 = gfs[1]  # 3/24/21 12:45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent change over time\n",
    "gf3 = (gf2 - gf) / gf\n",
    "\n",
    "# Use Python to scale the \"time (inc)\" column by 100%\n",
    "gf3.dataframe[\"time (inc)\"] *= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python to add a new column to the dataframe, computing the absolute value of the \"time (inc)\" column\n",
    "gf3.dataframe[\"abs-pct-change\"] = abs(gf3.dataframe[\"time (inc)\"])\n",
    "\n",
    "# Print the resulting tree\n",
    "# Notice that MPI node is red, indicating a high percent change in time\n",
    "print(gf3.tree(metric_column=\"abs-pct-change\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting dataframe of metrics\n",
    "# The dataframe now has the new column that was created earlier called \"abs-pct-change\"\n",
    "display(HTML(gf3.dataframe.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick analysis of percent change over time\n",
    "\n",
    "We're interested in analyzing how the MPI library changed overtime as compared to the computation part of our application. For this, we will filter our percent change graphframe to look at the communication and the computation part of our code individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter for MPI function calls, apply this filter to the graphframe above\n",
    "filt_mpi_func = lambda x: x[\"name\"].startswith(\"MPI_\")\n",
    "filter_mpi = gf3.filter(filt_mpi_func, squash=True)\n",
    "\n",
    "# Print the resulting dataframe of metrics\n",
    "display(HTML(filter_mpi.dataframe.to_html()))\n",
    "\n",
    "# Calculate the average percent change across all MPI function calls\n",
    "print(\n",
    "    \"Avg percent change of MPI functions: {0}%\".format(\n",
    "        round(filter_mpi.dataframe[\"abs-pct-change\"].mean(), 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter for non-MPI function calls (e.g., computation), apply this filter to the graphframe above\n",
    "filt_non_mpi_func = lambda x: not x[\"name\"].startswith(\"MPI_\")\n",
    "filter_non_mpi = gf3.filter(filt_non_mpi_func, squash=True)\n",
    "\n",
    "# Print the resulting dataframe of metrics\n",
    "display(HTML(filter_non_mpi.dataframe.to_html()))\n",
    "\n",
    "# Calculate the average percent change across all non-MPI (e.g., computation) function calls\n",
    "print(\n",
    "    \"Avg percent change of computation functions: {0}%\".format(\n",
    "        round(filter_non_mpi.dataframe[\"abs-pct-change\"].mean(), 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate speedup between two trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to SPOT/Caliper files\n",
    "input_deploy_dir_str = \"./hatchet-data\"\n",
    "# Input file configurations:\n",
    "#    problem size=10, niter=215, jobsize=1\n",
    "#    problem size=10, niter=800, jobsize=64\n",
    "input_run_ids_str = \"cDPu64825TuLB5ujG_0.cali,cjDCIuaXAoayBi9Lr_2.cali\" \n",
    "\n",
    "db = spotdb.connect(input_deploy_dir_str)\n",
    "runs = input_run_ids_str.split(',')\n",
    "\n",
    "# Read SPOT/Caliper files into a Hatchet GraphFrame\n",
    "gfs = ht.GraphFrame.from_spotdb(db, runs)\n",
    "\n",
    "gf = gfs[0]   # 1 rank\n",
    "gf2 = gfs[1]  # 64 ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree representation using the default metric\n",
    "print(gf.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree representation using the metric\n",
    "print(gf2.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the speedup from 1 to 64 ranks\n",
    "gf3 = gf / gf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting tree\n",
    "# Two things to note here:\n",
    "# 1) The MPI nodes are annotated with a green arrow that points to the right. This indicates that those\n",
    "#    nodes exist only in the right tree (i.e., 64 ranks). By right tree, we are referring to the position\n",
    "#    in the equation gf3 = gf / gf2 as shown in the above cell.\n",
    "# 2) Nodes with good speedup are highlighted in red, but may be preferred to color these nodes in green.\n",
    "print(gf3.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print resulting tree, but reverse the color scheme, so red identifies nodes with poor scaling (low values)\n",
    "print(gf3.tree(invert_colormap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting dataframe of metrics\n",
    "display(HTML(gf3.dataframe.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a scaling plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "\n",
    "# problemsize=50, iter=800, jobsize={27, 64, 125, 216, 343}\n",
    "WEAK_SCALE_CALI_FILES = [\n",
    "    \"cH_Cf7SG68XmGTJie_1.cali\",\n",
    "    \"cfN2J75TCzYjMpOcE_1.cali\",\n",
    "    \"cSnFyF8eGAduI9X1A_1.cali\",\n",
    "    \"cPShLtn_8i6zbKQdD_1.cali\",\n",
    "    \"cQ-CGJlYj-uFT2yv-_1.cali\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "input_db_uri_str = \"./hatchet-data\"\n",
    "input_run_ids_str = \",\".join(WEAK_SCALE_CALI_FILES)\n",
    "\n",
    "db = spotdb.connect(input_db_uri_str)\n",
    "runs = input_run_ids_str.split(',')\n",
    "\n",
    "# Read SPOT/Caliper files into Hatchet GraphFrame\n",
    "gfs = ht.GraphFrame.from_spotdb(db, runs)\n",
    "\n",
    "for idx, gf in enumerate(gfs):\n",
    "    # Extract the number of ranks from the filename, and add this as a new column in the dataframe\n",
    "    jobsize = int(gf.metadata.get(\"jobsize\", 1))\n",
    "    gf.dataframe[\"nranks\"] = jobsize\n",
    "    \n",
    "    # Filter the dataframe to match `Calc*` functions that have a duration greater than 15 seconds\n",
    "    filtered_gf = gf.filter(lambda x: x[\"avg#inclusive#sum#time.duration\"] > 15 and x[\"name\"].startswith('Calc'))\n",
    "  \n",
    "    # Append the filtered dataframe to a global list of dataframes\n",
    "    dataframes.append(filtered_gf.dataframe)\n",
    "\n",
    "# Concatenate list of dataframes into a single dataframe\n",
    "result = pd.concat(dataframes)  \n",
    "\n",
    "# Format rank column with leading 0s\n",
    "result[\"nranks\"] = result[\"nranks\"].apply(lambda x: '{0:0>3}'.format(x))\n",
    "\n",
    "# Create a line plot of number of ranks vs. inclusive time by function name\n",
    "pivot_df = result.pivot(index=\"nranks\",\n",
    "                        columns=\"name\",\n",
    "                        values=\"time (inc)\")\n",
    "plt = pivot_df.loc[:,:].plot.line(figsize=(10, 7),\n",
    "                                  legend=True,\n",
    "                                  fontsize=\"large\")\n",
    "\n",
    "# Set the plot title and its font size\n",
    "plt.set_title(\"Lulesh Weak Scaling of \\\"Calc*\\\" Functions\\nProblem Size: 50x50x50\",\n",
    "              fontsize=\"x-large\")\n",
    "\n",
    "# Customize the legend, set the font size of the title and labels\n",
    "plt.legend(loc='center left',\n",
    "           bbox_to_anchor=(1, 0.5),\n",
    "           title=\"Function Name\",\n",
    "           fontsize=\"x-large\",\n",
    "           title_fontsize=\"x-large\")\n",
    "\n",
    "# Set the x-axis label and its font size\n",
    "plt.set_xlabel(\"Number of Ranks\",\n",
    "               fontsize=\"x-large\")\n",
    "\n",
    "# Set the y-axis label and its font size\n",
    "plt.set_ylabel(\"Inclusive Time (sec)\",\n",
    "               fontsize=\"x-large\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
